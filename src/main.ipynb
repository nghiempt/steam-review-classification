{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing the pandas library for data manipulation and analysis\n",
    "import numpy as np  # Importing the numpy library for numerical operations\n",
    "import string  # Importing string library which contains a number of functions to process standard python strings\n",
    "import re  # Importing re library for regular expression operations\n",
    "import nltk  # Importing nltk library to work with human language data (texts)\n",
    "import tensorflow as tf  # Importing TensorFlow to build machine learning models\n",
    "\n",
    "# Importing various functions and classes to process text and build machine learning models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    log_loss,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Downloading necessary datasets for nltk\n",
    "nltk.download('punkt')  # Tokenizers for various language corpora\n",
    "nltk.download('stopwords')  # Common words that are usually ignored in text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from a CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('../resources/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring all text reviews are of string type for consistent processing\n",
    "X = data['review_text'].astype(str)\n",
    "\n",
    "# Converting negative review scores to zero as part of data cleaning\n",
    "data['review_score'] = data['review_score'].replace(-1, 0)\n",
    "\n",
    "# The target variable indicating if the review is positive or negative\n",
    "y = data['review_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Preprocessing function to clean and prepare text data\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    tokens = word_tokenize(text.lower())  # Tokenizing text and converting to lower case\n",
    "\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Removing punctuation and non-alphabetic characters\n",
    "\n",
    "    stop_words = set(stopwords.words('vietnamese'))  # Setting stopwords for Vietnamese (replace if using another language)\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Removing stopwords from tokens\n",
    "\n",
    "    stemmer = PorterStemmer()  # Initializing stemmer to reduce words to their root form\n",
    "    tokens = [stemmer.stem(word) for word in tokens]  # Stemming each word in tokens\n",
    "\n",
    "    return ' '.join(tokens)  # Joining processed tokens back into a single string\n",
    "\n",
    "# Applying text preprocessing to review texts\n",
    "X = X.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing between CountVectorizer (Bag of Words) or TfidfVectorizer (TF-IDF features)\n",
    "vectorizer = TfidfVectorizer()  # Initializing the TfidfVectorizer\n",
    "X_vectorized = vectorizer.fit_transform(X)  # Transforming the text to a vectorized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing machine learning models to be trained\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GBM': GradientBoostingClassifier(),\n",
    "    'NaiveBayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "results = {}  # Dictionary to store results of model evaluation\n",
    "\n",
    "# Looping through the models, training them, and evaluating performance\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)  # Training the model\n",
    "    y_pred = model.predict(X_test)  # Making predictions on the test set\n",
    "    # Getting probabilities for the positive class or decision function scores\n",
    "    y_proba = model.predict_proba(X_test)[:,1] if hasattr(model, \"predict_proba\") else model.decision_function(X_test)\n",
    "    \n",
    "    # Calculating different performance metrics for model evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_proba)\n",
    "    auc_pr = np.trapz(recall_curve, precision_curve)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    cross_entropy_loss = log_loss(y_test, y_proba)\n",
    "    \n",
    "    # Storing the calculated metrics in the results dictionary\n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'AUC-ROC': roc_auc,\n",
    "        'AUC-PR': auc_pr,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'MCC': mcc,\n",
    "        'Log Loss': cross_entropy_loss,\n",
    "    }\n",
    "\n",
    "# Printing out the results for each model\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name} results:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting sparse matrices to dense arrays if necessary for neural network input\n",
    "X_train_dense = X_train.toarray() if 'toarray' in dir(X_train) else X_train\n",
    "X_test_dense = X_test.toarray() if 'toarray' in dir(X_test) else X_test\n",
    "\n",
    "# Defining a simple neural network model using Keras\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(64, activation='relu', input_shape=(X_train_dense.shape[1],)))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the neural network model\n",
    "nn_model.fit(X_train_dense, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluating the neural network model\n",
    "y_pred_nn = (nn_model.predict(X_test_dense) > 0.5).astype(int)\n",
    "y_proba_nn = nn_model.predict(X_test_dense).flatten()\n",
    "\n",
    "# Calculating performance metrics for the neural network model\n",
    "nn_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_nn),\n",
    "    'Precision': precision_score(y_test, y_pred_nn),\n",
    "    'Recall': recall_score(y_test, y_pred_nn),\n",
    "    'F1 Score': f1_score(y_test, y_pred_nn),\n",
    "    'AUC-ROC': roc_auc_score(y_test, y_proba_nn),\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred_nn),\n",
    "    'MCC': matthews_corrcoef(y_test, y_pred_nn),\n",
    "    'Log Loss': log_loss(y_test, y_proba_nn),\n",
    "}\n",
    "\n",
    "results['NeuralNetwork'] = nn_metrics  # Adding neural network metrics to the results dictionary\n",
    "\n",
    "# Printing out the neural network results\n",
    "print(\"Neural Network results:\")\n",
    "for metric_name, metric_value in nn_metrics.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
